#!/usr/bin/env python
# coding: utf-8

# In[11]:


# -*- coding: utf-8 -*-
"""Problem.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15M_Ihj85vCwkJjy1eDq2iMkgiGAYW3f2
"""

import numpy as np
# The actual target value, used to calculate the error
target_actual = 3
input_data = np.array([0, 3])
# Sample weights
weights_0 = {'node_0': [2, 1],
             'node_1': [1, 2],
             'output': [1, 1]
            }

def relu(input):
    '''Define your relu activation function here'''
    # Calculate the value for the output of the relu function: output
    output = max(0, input)
    
    # Return the value just calculated
    return(output)

# Calculate node 0 value: node_0_output
node_0_input = (input_data * weights_0['node_0']).sum()
node_0_output = relu(node_0_input)

# Calculate node 1 value: node_1_output
node_1_input = (input_data * weights_0['node_1']).sum()
node_1_output = relu(node_1_input)

# Put node values into array: hidden_layer_outputs
hidden_layer_outputs = np.array([node_0_output, node_1_output])

# Calculate model output (do not apply relu)
model_output = (hidden_layer_outputs * weights_0['output']).sum()

# Print model output
print("Before training: predicited output is:", model_output)

# Calculate error: error_0
error_0 = model_output - target_actual
print("before weight optimize , error before optmizing the model", error_0)

"""Weight Updates:
---
These weight updates by the manually but this is not good apporach.
we are dealing with different methods for updating paramenters.

"""

# update weights for optimizing the model (this is done through )
# Create weights that cause the network to make perfect prediction (3): weights_1
weights_1 = {'node_0': [1, 1],
             'node_1': [1, 2],
             'output': [1, 0]
            }

# Calculate node 0 value: node_0_output
node_0_input_1 = (input_data * weights_1['node_0']).sum()
node_0_output_1 = relu(node_0_input_1)

# Calculate node 1 value: node_1_output
node_1_input_1 = (input_data * weights_1['node_1']).sum()
node_1_output_1 = relu(node_1_input_1)

# Put node values into array: hidden_layer_outputs
hidden_layer_outputs_1 = np.array([node_0_output_1, node_1_output_1])

# Calculate model output (do not apply relu)
model_output_1 = (hidden_layer_outputs_1 * weights_1['output']).sum()

# Print model output
print("After updating weight and model is  and give the same output as an actual output:", model_output_1)

# Calculate error: error_0
error_1 = model_output_1 - target_actual
print("Error after model optmiizing :",error_1)


# In[8]:


# -*- coding: utf-8 -*-
"""Backpropogation (Final Exam ).ipynb

Automatically generated by Colaboratory.

1. Loss / cost Function (MSE)
2. Gradient Decent
3. Derivates (Chain Rules)

Original file is located at
    https://colab.research.google.com/drive/1UwSegk22FQY87-28W6Yq1vHKQwHIA--O

Feedforward and backward
---
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd

import matplotlib.pyplot as plt
import seaborn as sns

sns.set_style("darkgrid")
# %matplotlib inline

df = pd.read_csv("https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data",names=['sepal_length','sepal_width','petal_length','petal_width','result'], header=None)
df = df.iloc[[1, 51, 101]]
df

"""Data preparation:
---
"""

# one-hot encoding
y = pd.get_dummies(df.result).values

N = y.size

x = df.drop(["result"], axis=1).values

print("Shape of the output value: \n\n",y.shape)
print("\nOne hot encoding Output\n\n ",y)

print("\n\n Size of N",N)

print("\n Labeled Feature",x)

"""Activation Function:
---
"""

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

"""
Hyperparameters:
---
"""

learning_rate = 0.1

n_input = 4
n_hidden = 2
n_output = 3

np.random.seed(10)

weights_1 = np.random.normal(scale=0.5, size=(n_input, n_hidden))   # (4, 2)
weights_2 = np.random.normal(scale=0.5, size=(n_hidden, n_output))  # (2, 3)
print(weights_1)
print("")
print(weights_2)

"""Feedforward:
---
"""

# feedforward
hidden_layer_inputs = np.dot(x, weights_1)
hidden_layer_outputs = sigmoid(hidden_layer_inputs)

output_layer_inputs = np.dot(hidden_layer_outputs, weights_2)
output_layer_outputs = sigmoid(output_layer_inputs)

#error at output layer
#Error calculation at the output layer
output_layer_error = output_layer_outputs - y
print(output_layer_error)

"""Backpropagation:
---
"""

# backpropagation: here we can calculate the error on output layer and also on the hidden layer.
output_layer_delta = output_layer_error * output_layer_outputs * (1 - output_layer_outputs)
# T show the transpose of weights and inputs as per used with any term
hidden_layer_error = np.dot(output_layer_delta, weights_2.T)
hidden_layer_delta = hidden_layer_error * hidden_layer_outputs * (1 - hidden_layer_outputs)

"""Weights updated:
---
"""

# weight updates w r t MSE
weights_2_update = np.dot(hidden_layer_outputs.T, output_layer_delta) / N
weights_1_update = np.dot(x.T, hidden_layer_delta) / N

#update w r t Gradient decent apporach
weights_2 = weights_2 - learning_rate * weights_2_update
weights_1 = weights_1 - learning_rate * weights_1_update
print(weights_2)
print("")
print(weights_1)

"""Mean Saqure Error before weight update:
---
"""

mse = ((output_layer_outputs - y)**2).sum() / (2*N)
mse

"""Mean Saqure Error after updated weights:
---
"""

# feedforward
# after updated weight we can predict the model again and check the mse if it reduced then it will work
hidden_layer_inputs = np.dot(x, weights_1)
hidden_layer_outputs = sigmoid(hidden_layer_inputs)

output_layer_inputs = np.dot(hidden_layer_outputs, weights_2)
output_layer_outputs = sigmoid(output_layer_inputs)

mse = ((output_layer_outputs - y)**2).sum() / (2*N)
mse

"""Result of 1 epoch:
---
According the mse output it shown the small level of update in the loss function but we will need to more minimum that predict more accurately.

Gradient Decent:
---
"""


# In[9]:



"""Result of 1 epoch:
---
According the mse output it shown the small level of update in the loss function but we will need to more minimum that predict more accurately.

Gradient Decent:
---
"""

def f(x):
    return x**2

def f_prime(x):
    return 2 * x

x = np.arange(-10, 10.01, 0.01)
df = pd.DataFrame({"y": f(x)}, index=x)
df

def plot_gradient_descent(starting_x, learning_rate, n_gradient_descent_steps):

    # gradient descent steps
    steps = [starting_x]
    for step in range(n_gradient_descent_steps):
        current_step = steps[-1]
        next_step = current_step - learning_rate * f_prime(current_step)
        steps.append(next_step)

    steps = np.array(steps)
    last_step = round(steps[-1], ndigits=2)
    df_gradient_descent = pd.DataFrame({"y": f(steps)})

    # plots
    fig, axes = plt.subplots(1, 2, figsize=(20, 6))
    plot_1, plot_2 = axes

    df.plot(legend=False, ax=plot_1)
    plot_1.plot(steps, f(steps), linestyle='--', marker='o', color='k')
    plot_1.set_xlabel("x")
    plot_1.set_ylabel("f(x)")
    plot_1.set_title("f(x) = xÂ²", fontsize=16)
    plot_1.annotate("learning rate = {}".format(learning_rate), (-2, 90), fontsize=14)
    plot_1.annotate(f"Value of x at the last step: {last_step}", (-4, 85), fontsize=14)

    df_gradient_descent.plot(legend=False, ax=plot_2)
    plot_2.set_ylabel("f(x)")
    plot_2.set_xlabel("Gradient Descent Step")
    plot_2.set_title("f(x) based on Gradient Descent Steps", fontsize=16)
    plot_2.annotate("learning rate = {}".format(learning_rate), (len(steps) / 2, f(steps[1])), fontsize=14)
    
    return

plot_gradient_descent(starting_x=3, learning_rate=0.1, n_gradient_descent_steps=15)

plot_gradient_descent(starting_x=3, learning_rate=0.2, n_gradient_descent_steps=10)

#plot_gradient_descent(starting_x=5, learning_rate=0.2, n_gradient_descent_steps=100)
plot_gradient_descent(starting_x=5, learning_rate=0.002, n_gradient_descent_steps=100)

plot_gradient_descent(starting_x=5, learning_rate=0.002, n_gradient_descent_steps=1000)

plot_gradient_descent(starting_x=3, learning_rate=0.9, n_gradient_descent_steps=10)

plot_gradient_descent(starting_x=3, learning_rate=1.1, n_gradient_descent_steps=3)

